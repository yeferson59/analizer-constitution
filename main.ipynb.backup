{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a633aa",
   "metadata": {},
   "source": [
    "# Asistente Legal (LangChain + LLM)\n",
    "\n",
    "Este notebook implementa el proyecto: \"Asistente legal / normativo\" para consultar leyes colombianas.\n",
    "\n",
    "Objetivo de esta sección: preparar el entorno e importar las librerías necesarias. Sigue las celdas en orden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea98b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.75 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-groq) (0.3.76)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.4.30)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\entorno_notebook\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "##!uv add \"langchain[openai]\" dotenv ipywidgets chromadb faiss-cpu PyPDF2\n",
    "\n",
    "## installar dependencias con pip\n",
    "#%pip install -q \"langchain[openai]\" python-dotenv ipywidgets chromadb faiss-cpu PyPDF2\n",
    "#!pip install groq langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1edf3d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Justin Bieber nació el 1 de marzo de 1994. El Super Bowl XXVIII se jugó en 1994 y fue ganado por el equipo de la NFL Dallas Cowboys.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 57, 'total_tokens': 99, 'completion_time': 0.065111534, 'prompt_time': 0.004080973, 'queue_time': 0.192308378, 'total_time': 0.069192507}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_50a6be1b6f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--66095e30-4306-4c5d-a5fe-c8a13ad2b4a2-0' usage_metadata={'input_tokens': 57, 'output_tokens': 42, 'total_tokens': 99}\n"
     ]
    }
   ],
   "source": [
    "# Imports y configuración inicial\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt template de ejemplo\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Inicialización del LLM (no ejecutar si no tiene las variables de entorno configuradas)\n",
    "#llm = ChatOpenAI(\n",
    " #   api_key=getenv(\"OPENAPI_API_KEY\"),\n",
    "  #  base_url=getenv(\"OPENAPI_BASE_URL\"),\n",
    "   # model=\"google/gemini-2.5-flash-lite-preview-09-2025\",\n",
    "#)\n",
    "llm = ChatGroq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model=\"llama-3.1-8b-instant\"  # Groq soporta modelos como llama3-8b-8192 o mixtral-8x7b\n",
    ")\n",
    "# Ejemplo de uso (descomente si tiene las claves configuradas)\n",
    "#question = \"¿Qué equipo de la NFL ganó el Super Bowl en el año en que nació Justin Bieber?\"\n",
    "#print(llm.invoke(input=question, config={\"prompt\": prompt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db183160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8840041487f4660bfd7262a4e300780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Sube archivos legales (.pdf, .txt, .md)'), FileUpload(value=(), accept='.txt,.md,.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdf781a79ce4a21a44fab2453cdf68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4561855c764b10a4282b52788f2e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Previsualizar archivos', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loaders: widget FileUpload y helpers para parsear archivos subidos y leer `data/`\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# Widget para subir archivos\n",
    "uploader = widgets.FileUpload(accept='.txt,.md,.pdf', multiple=True)\n",
    "display(widgets.VBox([widgets.Label(\"Sube archivos legales (.pdf, .txt, .md)\"), uploader]))\n",
    "\n",
    "# Output para previsualización\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "\n",
    "def parse_uploaded_files(uploader_widget):\n",
    "    \"\"\"Devuelve una lista de dicts {'filename', 'text'} extraídos de los archivos subidos.\"\"\"\n",
    "    docs = []\n",
    "    # La estructura de uploader.value depende del frontend; manejar ambos casos\n",
    "    items = getattr(uploader_widget, 'value', {}) or {}\n",
    "    # En algunos entornos items es lista, en otros dict\n",
    "    try:\n",
    "        iterator = items.items()\n",
    "    except Exception:\n",
    "        # intentamos tratar como lista\n",
    "        iterator = [(f.get('name', f\"file_{i}\"), f) for i, f in enumerate(items)]\n",
    "\n",
    "    for name, fileinfo in iterator:\n",
    "        # fileinfo puede ser dict con 'content' o directamente bytes\n",
    "        content = fileinfo.get('content') if isinstance(fileinfo, dict) else fileinfo\n",
    "        text = ''\n",
    "        if name.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                from PyPDF2 import PdfReader\n",
    "                reader = PdfReader(BytesIO(content))\n",
    "                pages = [p.extract_text() or '' for p in reader.pages]\n",
    "                text = '\\n'.join(pages)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] No se pudo leer PDF ({name}): {e}\"\n",
    "        else:\n",
    "            try:\n",
    "                if isinstance(content, bytes):\n",
    "                    text = content.decode('utf-8')\n",
    "                else:\n",
    "                    text = str(content)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] decodificando {name}: {e}\"\n",
    "        docs.append({'filename': name, 'text': text})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def on_preview_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        docs = parse_uploaded_files(uploader)\n",
    "        if not docs:\n",
    "            print('No se han encontrado archivos subidos.')\n",
    "            return\n",
    "        for d in docs:\n",
    "            print(f\"--- {d['filename']} ---\")\n",
    "            print(d['text'][:1500])\n",
    "            print('\\n')\n",
    "\n",
    "preview_btn = widgets.Button(description='Previsualizar archivos')\n",
    "preview_btn.on_click(on_preview_clicked)\n",
    "display(preview_btn)\n",
    "\n",
    "\n",
    "def load_local_docs(data_dir='data'):\n",
    "    \"\"\"Lee archivos de `data/` y devuelve lista de dicts {'filename','text'}\"\"\"\n",
    "    docs = []\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"No existe la carpeta '{data_dir}'. Crea la carpeta y añade documentos de ejemplo.\")\n",
    "        return docs\n",
    "    for fn in sorted(os.listdir(data_dir)):\n",
    "        path = os.path.join(data_dir, fn)\n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "        text = ''\n",
    "        if fn.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                from PyPDF2 import PdfReader\n",
    "                reader = PdfReader(path)\n",
    "                pages = [p.extract_text() or '' for p in reader.pages]\n",
    "                text = '\\n'.join(pages)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] leyendo PDF {fn}: {e}\"\n",
    "        else:\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] leyendo {fn}: {e}\"\n",
    "        docs.append({'filename': fn, 'text': text})\n",
    "    return docs\n",
    "\n",
    "# Ejemplo de uso: docs = load_local_docs('data')\n",
    "# print(docs[0]['text'][:500]) if docs else print('No hay documentos en data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45c55f",
   "metadata": {},
   "source": [
    "## Sección: Loaders (carga de documentos)\n",
    "\n",
    "En esta sección implementaremos los loaders que permiten: (1) subir archivos locales mediante un widget, y (2) cargar archivos desde la carpeta `data/` del repositorio. Más adelante convertiremos estos textos en chunks y los indexaremos en una vector DB.\n",
    "\n",
    "Instrucciones rápidas:\n",
    "- Usa el widget para subir `.pdf`, `.txt` o `.md`.\n",
    "- Pulsa \"Previsualizar archivos\" para ver el texto extraído (o errores si falta una dependencia de PDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a8d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object ID 7606,0 ref repaired\n",
      "Object ID 1,0 ref repaired\n",
      "Object ID 2,0 ref repaired\n",
      "Object ID 7582,0 ref repaired\n",
      "Object ID 7584,0 ref repaired\n",
      "Object ID 7583,0 ref repaired\n",
      "Object ID 7589,0 ref repaired\n",
      "Object ID 7591,0 ref repaired\n",
      "Object ID 7590,0 ref repaired\n",
      "Object ID 7596,0 ref repaired\n",
      "Object ID 7598,0 ref repaired\n",
      "Object ID 7597,0 ref repaired\n",
      "Object ID 4,0 ref repaired\n",
      "Object ID 7604,0 ref repaired\n",
      "Object ID 6,0 ref repaired\n",
      "Object ID 8,0 ref repaired\n",
      "Object ID 10,0 ref repaired\n",
      "Object ID 12,0 ref repaired\n",
      "Object ID 14,0 ref repaired\n",
      "Object ID 16,0 ref repaired\n",
      "Object ID 18,0 ref repaired\n",
      "Object ID 20,0 ref repaired\n",
      "Object ID 22,0 ref repaired\n",
      "Object ID 24,0 ref repaired\n",
      "Object ID 26,0 ref repaired\n",
      "Object ID 28,0 ref repaired\n",
      "Object ID 30,0 ref repaired\n",
      "Object ID 32,0 ref repaired\n",
      "Object ID 34,0 ref repaired\n",
      "Object ID 36,0 ref repaired\n",
      "Object ID 38,0 ref repaired\n",
      "Object ID 40,0 ref repaired\n",
      "Object ID 42,0 ref repaired\n",
      "Object ID 44,0 ref repaired\n",
      "Object ID 46,0 ref repaired\n",
      "Object ID 48,0 ref repaired\n",
      "Object ID 50,0 ref repaired\n",
      "Object ID 52,0 ref repaired\n",
      "Object ID 54,0 ref repaired\n",
      "Object ID 56,0 ref repaired\n",
      "Object ID 58,0 ref repaired\n",
      "Object ID 60,0 ref repaired\n",
      "Object ID 62,0 ref repaired\n",
      "Object ID 64,0 ref repaired\n",
      "Object ID 66,0 ref repaired\n",
      "Object ID 68,0 ref repaired\n",
      "Object ID 70,0 ref repaired\n",
      "Object ID 72,0 ref repaired\n",
      "Object ID 74,0 ref repaired\n",
      "Object ID 76,0 ref repaired\n",
      "Object ID 78,0 ref repaired\n",
      "Object ID 80,0 ref repaired\n",
      "Object ID 82,0 ref repaired\n",
      "Object ID 84,0 ref repaired\n",
      "Object ID 86,0 ref repaired\n",
      "Object ID 88,0 ref repaired\n",
      "Object ID 90,0 ref repaired\n",
      "Object ID 92,0 ref repaired\n",
      "Object ID 94,0 ref repaired\n",
      "Object ID 96,0 ref repaired\n",
      "Object ID 98,0 ref repaired\n",
      "Object ID 100,0 ref repaired\n",
      "Object ID 102,0 ref repaired\n",
      "Object ID 104,0 ref repaired\n",
      "Object ID 106,0 ref repaired\n",
      "Object ID 108,0 ref repaired\n",
      "Object ID 110,0 ref repaired\n",
      "Object ID 112,0 ref repaired\n",
      "Object ID 114,0 ref repaired\n",
      "Object ID 116,0 ref repaired\n",
      "Object ID 118,0 ref repaired\n",
      "Object ID 120,0 ref repaired\n",
      "Object ID 122,0 ref repaired\n",
      "Object ID 124,0 ref repaired\n",
      "Object ID 126,0 ref repaired\n",
      "Object ID 128,0 ref repaired\n",
      "Object ID 130,0 ref repaired\n",
      "Object ID 132,0 ref repaired\n",
      "Object ID 134,0 ref repaired\n",
      "Object ID 136,0 ref repaired\n",
      "Object ID 138,0 ref repaired\n",
      "Object ID 140,0 ref repaired\n",
      "Object ID 142,0 ref repaired\n",
      "Object ID 144,0 ref repaired\n",
      "Object ID 146,0 ref repaired\n",
      "Object ID 148,0 ref repaired\n",
      "Object ID 150,0 ref repaired\n",
      "Object ID 152,0 ref repaired\n",
      "Object ID 154,0 ref repaired\n",
      "Object ID 156,0 ref repaired\n",
      "Object ID 158,0 ref repaired\n",
      "Object ID 160,0 ref repaired\n",
      "Object ID 162,0 ref repaired\n",
      "Object ID 164,0 ref repaired\n",
      "Object ID 166,0 ref repaired\n",
      "Object ID 168,0 ref repaired\n",
      "Object ID 170,0 ref repaired\n",
      "Object ID 172,0 ref repaired\n",
      "Object ID 174,0 ref repaired\n",
      "Object ID 176,0 ref repaired\n",
      "Object ID 178,0 ref repaired\n",
      "Object ID 180,0 ref repaired\n",
      "Object ID 182,0 ref repaired\n",
      "Object ID 184,0 ref repaired\n",
      "Object ID 186,0 ref repaired\n",
      "Object ID 188,0 ref repaired\n",
      "Object ID 190,0 ref repaired\n",
      "Object ID 192,0 ref repaired\n",
      "Object ID 194,0 ref repaired\n",
      "Object ID 196,0 ref repaired\n",
      "Object ID 198,0 ref repaired\n",
      "Object ID 200,0 ref repaired\n",
      "Object ID 202,0 ref repaired\n",
      "Object ID 204,0 ref repaired\n",
      "Object ID 206,0 ref repaired\n",
      "Object ID 208,0 ref repaired\n",
      "Object ID 210,0 ref repaired\n",
      "Object ID 212,0 ref repaired\n",
      "Object ID 214,0 ref repaired\n",
      "Object ID 216,0 ref repaired\n",
      "Object ID 218,0 ref repaired\n",
      "Object ID 220,0 ref repaired\n",
      "Object ID 222,0 ref repaired\n",
      "Object ID 224,0 ref repaired\n",
      "Object ID 226,0 ref repaired\n",
      "Object ID 228,0 ref repaired\n",
      "Object ID 230,0 ref repaired\n",
      "Object ID 232,0 ref repaired\n",
      "Object ID 234,0 ref repaired\n",
      "Object ID 236,0 ref repaired\n",
      "Object ID 238,0 ref repaired\n",
      "Object ID 240,0 ref repaired\n",
      "Object ID 242,0 ref repaired\n",
      "Object ID 244,0 ref repaired\n",
      "Object ID 246,0 ref repaired\n",
      "Object ID 248,0 ref repaired\n",
      "Object ID 250,0 ref repaired\n",
      "Object ID 252,0 ref repaired\n",
      "Object ID 254,0 ref repaired\n",
      "Object ID 256,0 ref repaired\n",
      "Object ID 258,0 ref repaired\n",
      "Object ID 260,0 ref repaired\n",
      "Object ID 262,0 ref repaired\n",
      "Object ID 264,0 ref repaired\n",
      "Object ID 266,0 ref repaired\n",
      "Object ID 268,0 ref repaired\n",
      "Object ID 270,0 ref repaired\n",
      "Object ID 272,0 ref repaired\n",
      "Object ID 274,0 ref repaired\n",
      "Object ID 276,0 ref repaired\n",
      "Object ID 278,0 ref repaired\n",
      "Object ID 280,0 ref repaired\n",
      "Object ID 282,0 ref repaired\n",
      "Object ID 284,0 ref repaired\n",
      "Object ID 286,0 ref repaired\n",
      "Object ID 288,0 ref repaired\n",
      "Object ID 290,0 ref repaired\n",
      "Object ID 292,0 ref repaired\n",
      "Object ID 294,0 ref repaired\n",
      "Object ID 296,0 ref repaired\n",
      "Object ID 298,0 ref repaired\n",
      "Object ID 300,0 ref repaired\n",
      "Object ID 302,0 ref repaired\n",
      "Object ID 304,0 ref repaired\n",
      "Object ID 306,0 ref repaired\n",
      "Object ID 308,0 ref repaired\n",
      "Object ID 310,0 ref repaired\n",
      "Object ID 312,0 ref repaired\n",
      "Object ID 314,0 ref repaired\n",
      "Object ID 316,0 ref repaired\n",
      "Object ID 318,0 ref repaired\n",
      "Object ID 320,0 ref repaired\n",
      "Object ID 322,0 ref repaired\n",
      "Object ID 324,0 ref repaired\n",
      "Object ID 326,0 ref repaired\n",
      "Object ID 328,0 ref repaired\n",
      "Object ID 330,0 ref repaired\n",
      "Object ID 332,0 ref repaired\n",
      "Object ID 334,0 ref repaired\n",
      "Object ID 336,0 ref repaired\n",
      "Object ID 338,0 ref repaired\n",
      "Object ID 340,0 ref repaired\n",
      "Object ID 342,0 ref repaired\n",
      "Object ID 344,0 ref repaired\n",
      "Object ID 346,0 ref repaired\n",
      "Object ID 348,0 ref repaired\n",
      "Object ID 350,0 ref repaired\n",
      "Object ID 352,0 ref repaired\n",
      "Object ID 354,0 ref repaired\n",
      "Object ID 356,0 ref repaired\n",
      "Object ID 358,0 ref repaired\n",
      "Object ID 360,0 ref repaired\n",
      "Object ID 362,0 ref repaired\n",
      "Object ID 364,0 ref repaired\n",
      "Object ID 366,0 ref repaired\n",
      "Object ID 368,0 ref repaired\n",
      "Object ID 370,0 ref repaired\n",
      "Object ID 372,0 ref repaired\n",
      "Object ID 374,0 ref repaired\n",
      "Object ID 376,0 ref repaired\n",
      "Object ID 378,0 ref repaired\n",
      "Object ID 380,0 ref repaired\n",
      "Object ID 382,0 ref repaired\n",
      "Object ID 384,0 ref repaired\n",
      "Object ID 386,0 ref repaired\n",
      "Object ID 388,0 ref repaired\n",
      "Object ID 390,0 ref repaired\n",
      "Object ID 392,0 ref repaired\n",
      "Object ID 394,0 ref repaired\n",
      "Object ID 396,0 ref repaired\n",
      "Object ID 398,0 ref repaired\n",
      "Object ID 400,0 ref repaired\n",
      "Object ID 402,0 ref repaired\n",
      "Object ID 404,0 ref repaired\n",
      "Object ID 406,0 ref repaired\n",
      "Object ID 408,0 ref repaired\n",
      "Object ID 410,0 ref repaired\n",
      "Object ID 412,0 ref repaired\n",
      "Object ID 414,0 ref repaired\n",
      "Object ID 416,0 ref repaired\n",
      "Object ID 418,0 ref repaired\n",
      "Object ID 420,0 ref repaired\n",
      "Object ID 422,0 ref repaired\n",
      "Object ID 424,0 ref repaired\n",
      "Object ID 426,0 ref repaired\n",
      "Object ID 428,0 ref repaired\n",
      "Object ID 430,0 ref repaired\n",
      "Object ID 432,0 ref repaired\n",
      "Object ID 434,0 ref repaired\n",
      "Object ID 436,0 ref repaired\n",
      "Object ID 438,0 ref repaired\n",
      "Object ID 440,0 ref repaired\n",
      "Object ID 442,0 ref repaired\n",
      "Object ID 444,0 ref repaired\n",
      "Object ID 446,0 ref repaired\n",
      "Object ID 448,0 ref repaired\n",
      "Object ID 450,0 ref repaired\n",
      "Object ID 452,0 ref repaired\n",
      "Object ID 454,0 ref repaired\n",
      "Object ID 456,0 ref repaired\n",
      "Object ID 458,0 ref repaired\n",
      "Object ID 460,0 ref repaired\n",
      "Object ID 462,0 ref repaired\n",
      "Object ID 464,0 ref repaired\n",
      "Object ID 466,0 ref repaired\n",
      "Object ID 468,0 ref repaired\n",
      "Object ID 470,0 ref repaired\n",
      "Object ID 472,0 ref repaired\n",
      "Object ID 474,0 ref repaired\n",
      "Object ID 476,0 ref repaired\n",
      "Object ID 478,0 ref repaired\n",
      "Object ID 480,0 ref repaired\n",
      "Object ID 482,0 ref repaired\n",
      "Object ID 484,0 ref repaired\n",
      "Object ID 486,0 ref repaired\n",
      "Object ID 488,0 ref repaired\n",
      "Object ID 490,0 ref repaired\n",
      "Object ID 492,0 ref repaired\n",
      "Object ID 494,0 ref repaired\n",
      "Object ID 496,0 ref repaired\n",
      "Object ID 498,0 ref repaired\n",
      "Object ID 500,0 ref repaired\n",
      "Object ID 502,0 ref repaired\n",
      "Object ID 504,0 ref repaired\n",
      "Object ID 506,0 ref repaired\n",
      "Object ID 508,0 ref repaired\n",
      "Object ID 510,0 ref repaired\n",
      "Object ID 512,0 ref repaired\n",
      "Object ID 514,0 ref repaired\n",
      "Object ID 516,0 ref repaired\n",
      "Object ID 518,0 ref repaired\n",
      "Object ID 520,0 ref repaired\n",
      "Object ID 522,0 ref repaired\n",
      "Object ID 524,0 ref repaired\n",
      "Object ID 526,0 ref repaired\n",
      "Object ID 528,0 ref repaired\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos cargados: 2\n",
      "Total chunks generados: 1215\n",
      " - Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf: 1050 chunks\n",
      " - reglamento_academi_pregrado.pdf: 165 chunks\n",
      "\n",
      "Ejemplo (primer chunk, 500 chars):\n",
      "Departamento Administrativo de la Función Pública\n",
      "Constitución Política 1 de 1991 Asamblea\n",
      "Nacional Constituyente1 EVA - Gestor Normativo\n",
      "Constitución Política 1 de 1991 Asamblea Nacional\n",
      "Constituyente\n",
      "Los datos publicados tienen propósitos exclusivamente informativos. El Departamento Administrativo de la Función Pública no se hace\n",
      "responsable de la vigencia de la presente norma. Nos encontramos en un proceso permanente de actualización de los contenidos.\n",
      "CONSTITUCIÓN POLITICA DE LA REPUBLICA DE\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento: splitters y creación de chunks\n",
    "# Carga documentos desde `data/`, aplica un splitter y guarda los chunks en `DOCUMENT_CHUNKS`.\n",
    "from collections import Counter\n",
    "\n",
    "# Intentar importar el splitter de LangChain; si la API cambia, manejar el fallback\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "except Exception:\n",
    "    try:\n",
    "        # versión alternativa\n",
    "        from langchain.text_splitter import CharacterTextSplitter as RecursiveCharacterTextSplitter\n",
    "    except Exception:\n",
    "        RecursiveCharacterTextSplitter = None\n",
    "\n",
    "\n",
    "def create_chunks(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Devuelve lista de dicts {'source','chunk_index','text'}\"\"\"\n",
    "    if RecursiveCharacterTextSplitter is None:\n",
    "        raise ImportError(\"No se encontró RecursiveCharacterTextSplitter. Instala langchain actualizado.\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "    for d in docs:\n",
    "        text = d.get('text') or ''\n",
    "        if not text.strip():\n",
    "            # texto vacío o error al leer\n",
    "            continue\n",
    "        texts = splitter.split_text(text)\n",
    "        for i, t in enumerate(texts):\n",
    "            all_chunks.append({'source': d.get('filename', 'unknown'), 'chunk_index': i, 'text': t})\n",
    "    return all_chunks\n",
    "\n",
    "# Cargar documentos locales desde data/\n",
    "DOCUMENTS = load_local_docs('data')\n",
    "print(f\"Documentos cargados: {len(DOCUMENTS)}\")\n",
    "\n",
    "# Crear chunks\n",
    "DOCUMENT_CHUNKS = create_chunks(DOCUMENTS, chunk_size=1000, chunk_overlap=200) if DOCUMENTS else []\n",
    "print(f\"Total chunks generados: {len(DOCUMENT_CHUNKS)}\")\n",
    "\n",
    "# Mostrar conteo por documento y un ejemplo\n",
    "if DOCUMENT_CHUNKS:\n",
    "    per_doc = Counter([c['source'] for c in DOCUMENT_CHUNKS])\n",
    "    for doc, cnt in per_doc.items():\n",
    "        print(f\" - {doc}: {cnt} chunks\")\n",
    "    print('\\nEjemplo (primer chunk, 500 chars):')\n",
    "    print(DOCUMENT_CHUNKS[0]['text'][:500])\n",
    "else:\n",
    "    print('No hay chunks (documentos vacíos o problema al leer los archivos).')\n",
    "\n",
    "# Ahora `DOCUMENTS` y `DOCUMENT_CHUNKS` están listos para embeddings / indexado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7acfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\HP\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703e4b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'OpenAIEmbeddings' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m metadatas = [{\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: c[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m'\u001b[39m: c[\u001b[33m'\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m'\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m DOCUMENT_CHUNKS]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Inicializar embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m emb = \u001b[43mopenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Construir o cargar vectorstore\u001b[39;00m\n\u001b[32m     46\u001b[39m vectordb = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'OpenAIEmbeddings' object is not callable"
     ]
    }
   ],
   "source": [
    "# Embeddings + VectorStore (Chroma preferido, FAISS fallback) y pipeline RAG\n",
    "import textwrap\n",
    "# Requerir DOCUMENT_CHUNKS\n",
    "try:\n",
    "    len(DOCUMENT_CHUNKS)\n",
    "except NameError:\n",
    "    raise NameError('DOCUMENT_CHUNKS no está definido. Ejecuta la celda de splitters antes de esta.')\n",
    "\n",
    "# Import embeddings\n",
    "openAIEmbeddings = None\n",
    "try:\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    openAIEmbeddings = OpenAIEmbeddings()\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "        openAIEmbeddings = OpenAIEmbeddings()\n",
    "    except Exception:\n",
    "        openAIEmbeddings = None\n",
    "\n",
    "# Import vectorstores\n",
    "Chroma = None\n",
    "FAISS = None\n",
    "try:\n",
    "    from langchain.vectorstores import Chroma\n",
    "    Chroma = Chroma\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.vectorstores import FAISS\n",
    "        FAISS = FAISS\n",
    "    except Exception:\n",
    "        Chroma = None\n",
    "        FAISS = None\n",
    "\n",
    "if openAIEmbeddings is None:\n",
    "    raise ImportError('No se encontró OpenAIEmbeddings. Instala/actualiza langchain o ajusta el código según la versión.')\n",
    "\n",
    "# Preparar textos y metadatos\n",
    "texts = [c['text'] for c in DOCUMENT_CHUNKS]\n",
    "metadatas = [{'source': c['source'], 'chunk_index': c['chunk_index']} for c in DOCUMENT_CHUNKS]\n",
    "\n",
    "# Inicializar embeddings\n",
    "emb = openAIEmbeddings()\n",
    "\n",
    "# Construir o cargar vectorstore\n",
    "vectordb = None\n",
    "try:\n",
    "    if Chroma is not None:\n",
    "        # persist_directory crea/usa la carpeta chroma_db\n",
    "        vectordb = Chroma.from_texts(texts, embedding=emb, metadatas=metadatas, collection_name='laws', persist_directory='chroma_db')\n",
    "        try:\n",
    "            vectordb.persist()\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        # FAISS fallback (in-memory)\n",
    "        from langchain.vectorstores import FAISS\n",
    "        vectordb = FAISS.from_texts(texts, embedding=emb, metadatas=metadatas)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Error creando vectorstore: {e}')\n",
    "\n",
    "print('Vectorstore preparado.')\n",
    "\n",
    "# Helper: recuperar documentos (devuelve lista de objetos con page_content y metadata)\n",
    "def retrieve(query, k=4):\n",
    "    # similarity_search devuelve objetos Document o similares\n",
    "    try:\n",
    "        results = vectordb.similarity_search(query, k=k)\n",
    "    except Exception:\n",
    "        # Algunas versiones usan similarity_search_with_relevance_scores\n",
    "        try:\n",
    "            results = [r[0] for r in vectordb.similarity_search_with_relevance_scores(query, k=k)]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'No se pudo ejecutar la búsqueda: {e}')\n",
    "    return results\n",
    "\n",
    "# Pipeline RAG: sintetizar respuesta con LLM usando contexto recuperado\n",
    "\n",
    "def rag_answer(query, k=4):\n",
    "    docs = retrieve(query, k=k)\n",
    "    if not docs:\n",
    "        return {'answer': 'No se encontraron documentos relevantes.', 'citations': []}\n",
    "\n",
    "    # Construir contexto concatenando fragmentos recuperados con metadata\n",
    "    ctx_parts = []\n",
    "    citations = []\n",
    "    for d in docs:\n",
    "        # compatibilidad: Document tiene page_content y metadata\n",
    "        text = getattr(d, 'page_content', None) or d.get('text') if isinstance(d, dict) else str(d)\n",
    "        meta = getattr(d, 'metadata', None) or (d if isinstance(d, dict) else {})\n",
    "        src = meta.get('source', meta.get('file_name', 'unknown'))\n",
    "        idx = meta.get('chunk_index')\n",
    "        citations.append({'source': src, 'chunk_index': idx})\n",
    "        snippet = textwrap.shorten(text, width=1000, placeholder='...')\n",
    "        ctx_parts.append(f\"Source: {src} (chunk {idx})\\n{snippet}\")\n",
    "\n",
    "    context = '\\n\\n'.join(ctx_parts)\n",
    "\n",
    "    # Prompt final\n",
    "    final_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer conciso en español. Indica al final las citas como [source:filename chunk_index].\"\n",
    "\n",
    "    # Llamada al LLM (usar llm.invoke si está disponible)\n",
    "    try:\n",
    "        resp = llm.invoke(input=final_prompt)\n",
    "        # si resp es un objeto complejo, intentar extraer texto\n",
    "        answer_text = str(resp)\n",
    "    except Exception:\n",
    "        try:\n",
    "            # fallback: si llm tiene método __call__\n",
    "            answer_text = llm(final_prompt)\n",
    "        except Exception as e:\n",
    "            answer_text = f\"[ERROR llamando al LLM: {e}]\"\n",
    "\n",
    "    return {'answer': answer_text, 'citations': citations}\n",
    "\n",
    "# Widget simple para consultas RAG\n",
    "query_box = widgets.Text(placeholder='Escribe tu pregunta sobre la ley (ej: ¿Qué artículo protege la libertad de expresión?)', description='Pregunta:', layout=widgets.Layout(width='80%'))\n",
    "ask_btn = widgets.Button(description='Consultar')\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_ask(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        q = query_box.value.strip()\n",
    "        if not q:\n",
    "            print('Escribe una pregunta primero.')\n",
    "            return\n",
    "        print('Buscando documentos relevantes...')\n",
    "        res = rag_answer(q, k=4)\n",
    "        print('\\n--- RESPUESTA ---\\n')\n",
    "        print(res['answer'])\n",
    "        print('\\n--- CITAS ---')\n",
    "        for c in res['citations']:\n",
    "            print(f\"- {c['source']} (chunk {c['chunk_index']})\")\n",
    "\n",
    "ask_btn.on_click(on_ask)\n",
    "\n",
    "display(widgets.VBox([query_box, ask_btn, out]))\n",
    "\n",
    "print('Interfaz RAG lista. Escribe una pregunta y pulsa \"Consultar\".')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd5121dc-87da-4628-800b-d3de4eb4fea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando modelo: gpt-3.5-turbo\n",
      "LLM inicializado (ChatOpenAI). Probando llamada breve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18300\\761360218.py:25: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  prueba = llm(\"Saluda brevemente en español.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error inicializando ChatOpenAI (revisa modelo/API key): Error llamando al LLM tras inicializar: 'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     prueba = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSaluda brevemente en español.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     respuesta = \u001b[38;5;28mgetattr\u001b[39m(prueba, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(prueba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m config = ensure_config(config)\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1022\u001b[39m prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m     results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m            \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m     )\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1089\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1184\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1188\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1189\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1179\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m     raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     response = raw_response.parse()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1146\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     prueba = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSaluda brevemente en español.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     respuesta = \u001b[38;5;28mgetattr\u001b[39m(prueba, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(prueba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:193\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1315\u001b[39m, in \u001b[36mBaseChatModel.__call__\u001b[39m\u001b[34m(self, messages, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1297\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the model.\u001b[39;00m\n\u001b[32m   1298\u001b[39m \n\u001b[32m   1299\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1313\u001b[39m \n\u001b[32m   1314\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m generation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:821\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    812\u001b[39m callback_manager = CallbackManager.configure(\n\u001b[32m    813\u001b[39m     callbacks,\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata,\n\u001b[32m    820\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m messages_to_trace = \u001b[43m[\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_format_for_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    824\u001b[39m run_managers = callback_manager.on_chat_model_start(\n\u001b[32m    825\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    826\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     batch_size=\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[32m    832\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:822\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    812\u001b[39m callback_manager = CallbackManager.configure(\n\u001b[32m    813\u001b[39m     callbacks,\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata,\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m messages_to_trace = [\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[43m_format_for_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    823\u001b[39m ]\n\u001b[32m    824\u001b[39m run_managers = callback_manager.on_chat_model_start(\n\u001b[32m    825\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    826\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     batch_size=\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[32m    832\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\entorno_notebook\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:125\u001b[39m, in \u001b[36m_format_for_tracing\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    124\u001b[39m message_to_trace = message\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(message.content):\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'content'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError llamando al LLM tras inicializar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRespuesta de prueba:\u001b[39m\u001b[33m\"\u001b[39m, respuesta)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error llamando al LLM tras inicializar: 'str' object has no attribute 'content'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRespuesta de prueba:\u001b[39m\u001b[33m\"\u001b[39m, respuesta)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# captura y muestrame el error completo (útil para depuración)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError inicializando ChatOpenAI (revisa modelo/API key): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error inicializando ChatOpenAI (revisa modelo/API key): Error llamando al LLM tras inicializar: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# CELDA: inicializar ChatOpenAI (robusta ante distintos estilos de llamada)\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo\")  # cambia aquí si tienes otro modelo\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Usando modelo:\", MODEL)\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY no encontrada. Define la variable de entorno antes de continuar.\")\n",
    "\n",
    "# Intenta inicializar el cliente LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(api_key=API_KEY, model=MODEL)\n",
    "    print(\"LLM inicializado (ChatOpenAI). Probando llamada breve...\")\n",
    "    # prueba de llamada con distintos estilos de interfaz (invoke / __call__)\n",
    "    prueba = None\n",
    "    try:\n",
    "        prueba = llm.invoke(\"Saluda brevemente en español.\")\n",
    "        respuesta = getattr(prueba, \"content\", None) or str(prueba)\n",
    "    except Exception:\n",
    "        # fallback a llamada directa\n",
    "        try:\n",
    "            prueba = llm(\"Saluda brevemente en español.\")\n",
    "            respuesta = getattr(prueba, \"content\", None) or str(prueba)\n",
    "        except Exception as err:\n",
    "            raise RuntimeError(f\"Error llamando al LLM tras inicializar: {err}\")\n",
    "    print(\"Respuesta de prueba:\", respuesta)\n",
    "except Exception as e:\n",
    "    # captura y muestrame el error completo (útil para depuración)\n",
    "    raise RuntimeError(f\"Error inicializando ChatOpenAI (revisa modelo/API key): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748fbc7-c341-4bda-9a52-df0cfe6017ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##WILSON ROJAS HIZÓ DESDE ACÁ. LA API KEY DE CHAT OPENAI NO ME SIRVE ENTONCES USÉ LA DE GROQ. ARRIBA ESTA EL LLAMADO , SI VAN A\n",
    "##USAR LA DE CHATOPENAI , SOLO COMENTÉN MIS LINEAS DE GROQ , NO LAS ELIMINEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a79ae253-a871-489e-98b4-f1f3a1261f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta cruda del LLM ===\n",
      "```json\n",
      "{\n",
      "  \"answer\": \"El artículo 20 de la Constitución Política de Colombia protege la libertad de expresión.\",\n",
      "  \"citations\": \"[CNP:CONSTITUCION_DE_1991, art. 20]\",\n",
      "  \"follow_up\": \"true\"\n",
      "}\n",
      "```\n",
      "\n",
      "Nota: El artículo 20 de la Constitución Política de Colombia establece que \"Todas las personas tienen derecho a la libertad de expresión y de información, que incluye la libertad de buscar, recibir, recibir y difundir cualquier tipo de información y opiniones sin previas censura, sin sujeción a limitaciones de orden político, ideológico o religioso, ni a cualquier otra que no esté justificada por una ley que, entre otros requisitos, debe ser respetuosa de ese derecho y no afectar la fundamentales de la personalidad humana.\"\n",
      "\n",
      "=== Respuesta parseada (dict válido) ===\n",
      "{'answer': 'El artículo 20 de la Constitución Política de Colombia protege la libertad de expresión.', 'citations': '[CNP:CONSTITUCION_DE_1991, art. 20]', 'follow_up': 'true'}\n"
     ]
    }
   ],
   "source": [
    "##SECCION 5 PARSES \n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Definir los campos esperados en la salida\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"Respuesta concisa a la consulta legal en español.\"),\n",
    "    ResponseSchema(name=\"citations\", description=\"Lista de citas con formato [doc:filename, chunk:index].\"),\n",
    "    ResponseSchema(name=\"follow_up\", description=\"true/false indicando si el usuario debería hacer una pregunta de seguimiento.\"),\n",
    "]\n",
    "\n",
    "# Crear el parser estructurado\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Instrucciones para el modelo\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Prompt template que exige formato\n",
    "parser_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Eres un asistente legal colombiano. Responde siempre en formato JSON.\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Ejemplo de uso\n",
    "question = \"¿Qué artículo de la Constitución Colombiana protege la libertad de expresión?\"\n",
    "\n",
    "prompt_text = parser_prompt.format(question=question)\n",
    "response = llm.invoke(prompt_text)\n",
    "\n",
    "print(\"=== Respuesta cruda del LLM ===\")\n",
    "print(response.content)\n",
    "\n",
    "try:\n",
    "    parsed = output_parser.parse(response.content)\n",
    "    print(\"\\n=== Respuesta parseada (dict válido) ===\")\n",
    "    print(parsed)\n",
    "except Exception as e:\n",
    "    print(\"\\n[ERROR] No se pudo parsear la salida:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9630f26d-63d9-44f2-a752-ca4adb7f6072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turno 1:\n",
      "¡Hola Wilson! Me alegra conocerte. ¿En qué puedo ayudarte hoy? ¿Tienes alguna inquietud o pregunta relacionada con tu carrera o un tema en particular?\n",
      "\n",
      "Turno 2 (con memoria):\n",
      "¡Hola Wilson! Me alegra que hayas recordado tus datos. Según nuestro historial de conversación, tu nombre es Wilson y estudias Ingeniería de Sistemas. ¿Quieres hablar sobre algo relacionado a tu carrera o necesitas orientación sobre algún tema en particular?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Definir un prompt que incluya historial de conversación\n",
    "memory_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente legal colombiano.\n",
    "Responde en español de forma clara y breve.\n",
    "\n",
    "Historial de conversación:\n",
    "{history}\n",
    "\n",
    "Nueva pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Inicializar la memoria\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "# Construir el chain con memoria\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=memory_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# Ejemplo de interacción multi-turno\n",
    "print(\"Turno 1:\")\n",
    "resp1 = conversation_chain.run(\"Soy un estudiante de Ingenieria de sistemas y me llamo Wilson\")\n",
    "print(resp1)\n",
    "\n",
    "print(\"\\nTurno 2 (con memoria):\")\n",
    "resp2 = conversation_chain.run(\"¿Como me llamo y que estudio?\")\n",
    "print(resp2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "489ee03f-111e-46d2-b3d4-c4b75a78e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1: Pregunta legal (usa RAG)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: La pregunta se refiere a una ley o documento específico en Colombia, por lo que es probable que deba buscar esta información en una base de datos legal.\n",
      "Action: Buscar la información en la base de datos legal.\n",
      "Action Input: query=\"libertad de expresión en Colombia\"\u001b[0m\n",
      "Observation: Buscar la información en la base de datos legal. is not a valid tool, try one of [BusquedaLegalRAG, LLMGeneral].\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Entiendo el error. La herramienta correcta para buscar información legal en una base de datos es BusquedaLegalRAG. \n",
      "Action: Utilizar la herramienta BusquedaLegalRAG.\n",
      "Action Input: query=\"libertad de expresión en Colombia\".\u001b[0m\n",
      "Observation: Utilizar la herramienta BusquedaLegalRAG. is not a valid tool, try one of [BusquedaLegalRAG, LLMGeneral].\n",
      "Thought:\u001b[32;1m\u001b[1;3mParece que cometí un error en la notación. Aquí está la respuesta corregida:\n",
      "\n",
      "Question: ¿Qué artículo protege la libertad de expresión en Colombia?\n",
      "Thought: La pregunta se refiere a una ley o documento específico en Colombia, por lo que es probable que deba buscar esta información en una base de datos legal.\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: query=\"libertad de expresión en Colombia\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mRespuesta (RAG): Según la Constitución Política de Colombia de 1991, la libertad de expresión es un derecho fundamental que se encuentra protegido en el artículo 19 de la Declaración Universal de Derechos Humanos, pero no tenemos ese dato aquí. Sin embargo, podemos encontrar que el artículo 20 de la Constitución establece que \"Toda persona tiene derecho a la libertad de expresión. El ejercicio de este derecho no puede ser restringido más que en los casos establecidos en la ley y siempre que se respeten los derechos de los demás y los principios de la moral pública\".\n",
      "\n",
      "[0: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk 0]\n",
      "[1: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk 1]\n",
      "[2: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk 2]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n",
      "\n",
      "Ejemplo 2: Pregunta general (usa LLM)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: La pregunta parece estar relacionada con la literatura y la historia de Colombia, por lo que podría requerir información general.\n",
      "Action: LLMGeneral\n",
      "Action Input: ¿Quién fue Gabriel García Márquez?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mGabriel García Márquez fue un escritor, periodista y diplomático colombiano, considerado uno de los principales exponentes de la literatura latinoamericana del siglo XX. Nació el 6 de marzo de 1927 en Aracataca, Colombia, y murió el 17 de abril de 2014 en México D.F.\n",
      "\n",
      "García Márquez es conocido por su estilo literario único, que combina elementos de la realista magia, el surrealismo y el folklorismo. Su obra más famosa es \"Cien años de soledad\", publicada en 1967, que es considerada una de las mejores novelas del siglo XX y que ganó el Premio Nobel de Literatura en 1982.\n",
      "\n",
      "Entre sus obras destacan también \"El amor en los tiempos del cólera\" (1985), \"Doce cuentos peregrinos\" (1992), \"La incendiada\" (1989) y \"La hojarasca\" (1955). Su estilo literario ha influido en la obra de muchos escritores y ha sido traducida a más de 30 idiomas.\n",
      "\n",
      "García Márquez fue también un periodista y un diplomático activo. Fue corresponsal en Caracas, Bogotá y México, y también trabajó como embajador de Colombia en España y como representante especial de las Naciones Unidas en Guatemala.\n",
      "\n",
      "Es importante destacar que García Márquez fue un escritor que buscaba reflejar la realidad de América Latina y su cultura en su obra, y que fue un defensor de la libertad de expresión y de la justicia social. Su legado literario sigue siendo muy influyente en la literatura contemporánea.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La información proporcionada es suficiente para responder a la pregunta, pero sería interesante saber más sobre las obras de Gabriel García Márquez y su impacto en la literatura latinoamericana.\n",
      "\n",
      "Action: LLMGeneral\n",
      "Action Input: ¿Cuáles son las obras más destacadas de Gabriel García Márquez y su impacto en la literatura latinoamericana?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mGabriel García Márquez es uno de los escritores más influyentes y destacados de la literatura latinoamericana del siglo XX. Sus obras han tenido un impacto significativo en la literatura mundial y han contribuido a la consolidación de la obra maestra de la literatura latinoamericana conocida como la \"magia realista\".\n",
      "\n",
      "**Obras destacadas:**\n",
      "\n",
      "1. **\"Cien años de soledad\" (1967)**: Considerada una de las obras más importantes de la literatura latinoamericana, esta novela sigue la historia de la familia Buendía y su relación con la ciudad ficticia de Macondo. La novela explora temas como la historia, la identidad y la condición humana.\n",
      "2. **\"El amor en los tiempos del cólera\" (1985)**: Esta novela es una exploración de la relación entre los personajes titulares, Florentino Ariza y Fermina Daza, a lo largo de varias décadas. La novela es un viaje a través del amor y la vida en la Colombia del siglo XIX.\n",
      "3. **\"La hojarasca\" (1955)**: Esta novela es una crítica a la burguesía colombiana y la corrupción en la región. La historia sigue a un grupo de personajes que viven en la región del Magdalena Medio.\n",
      "4. **\"El amor ajeno\" (1949)**: Esta novela es una exploración de la relación entre un hombre y una mujer que se ven obligados a enfrentar la realidad de su relación.\n",
      "5. **\"La mala hora\" (1962)**: Esta novela es una crítica a la sociedad colombiana de la época y explora temas como la corrupción, la violencia y la injusticia.\n",
      "\n",
      "**Impacto en la literatura latinoamericana:**\n",
      "\n",
      "1. **Magia realista**: García Márquez es considerado uno de los padres de la magia realista, un movimiento literario que busca fusionar la realidad con la fantasía y la leyenda.\n",
      "2. **Renovación de la literatura latinoamericana**: García Márquez y otros escritores de la generación de los 50, como Carlos Fuentes y Mario Vargas Llosa, renovaron la literatura latinoamericana y la llevaron a un nuevo nivel de creatividad y originalidad.\n",
      "3. **Influencia en la literatura mundial**: La obra de García Márquez ha influido en la literatura mundial, desde la magia realista hasta la literatura contemporánea.\n",
      "4. **Premios y reconocimientos**: García Márquez recibió el Premio Nobel de Literatura en 1982, lo que lo convirtió en el primer latinoamericano en recibir este premio.\n",
      "5. **Influencia en la cultura popular**: La obra de García Márquez ha sido adaptada en varias películas, series de televisión y obras de teatro, lo que ha contribuido a su popularidad y reconocimiento en la cultura popular.\n",
      "\n",
      "En resumen, la obra de García Márquez ha tenido un impacto significativo en la literatura latinoamericana y en la literatura mundial en general. Su innovadora técnica de la magia realista y su capacidad para explorar temas universales han hecho que su obra sea fundamental para la literatura contemporánea.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La información proporcionada es suficiente para responder a la pregunta, pero sería interesante saber más sobre las obras de Gabriel García Márquez y su impacto en la literatura latinoamericana.\n",
      "\n",
      "Action: LLMGeneral\n",
      "Action Input: ¿Cuál es la influencia de la magia realista en la literatura latinoamericana?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLa magia realista es un estilo literario que ha tenido una gran influencia en la literatura latinoamericana, especialmente en la segunda mitad del siglo XX. Este estilo, desarrollado por autores como Gabriel García Márquez, Isabel Allende, Alejo Carpentier y Mario Vargas Llosa, entre otros, se caracteriza por la mezcla de elementos realistas y fantásticos, lo que permite a los autores explorar temas y realidades complejas de manera innovadora y creativa.\n",
      "\n",
      "La magia realista ha tenido una influencia significativa en la literatura latinoamericana de varias maneras:\n",
      "\n",
      "1. **Reconstrucción de la identidad cultural**: La magia realista ha permitido a los autores latinoamericanos explorar y reconstruir su identidad cultural, incorporando elementos de la historia, la mitología y la tradición oral en sus obras.\n",
      "2. **Representación de la realidad**: La magia realista ha permitido a los autores representar la realidad de manera más compleja y profunda, abordando temas como la pobreza, la injusticia, la violencia y la opresión de manera más cruda y honesta.\n",
      "3. **Innovación narrativa**: La magia realista ha permitido a los autores innovar en la narrativa, utilizando técnicas como la intertextualidad, la ironía y la parodia para crear un lenguaje literario más rico y complejo.\n",
      "4. **Globalización de la literatura latinoamericana**: La magia realista ha permitido a la literatura latinoamericana alcanzar un público más amplio y global, especialmente en Europa y Estados Unidos, donde la obra de autores como García Márquez y Allende ha sido ampliamente leída y reconocida.\n",
      "\n",
      "Algunos de los efectos más significativos de la magia realista en la literatura latinoamericana son:\n",
      "\n",
      "1. **La renovación del género de la novela**: La magia realista ha permitido a los autores latinoamericanos renovar el género de la novela, creando obras más complejas y innovadoras que las generaciones anteriores.\n",
      "2. **La creación de un estilo literario propio**: La magia realista ha permitido a los autores latinoamericanos crear un estilo literario propio, que se caracteriza por la mezcla de elementos realistas y fantásticos.\n",
      "3. **La expansión de la literatura latinoamericana**: La magia realista ha permitido a la literatura latinoamericana alcanzar un público más amplio y global, lo que ha llevado a una mayor reconocimiento y aceptación de la literatura latina como una parte importante de la literatura universal.\n",
      "\n",
      "En resumen, la magia realista ha tenido una influencia significativa en la literatura latinoamericana, permitiendo a los autores explorar temas y realidades complejas de manera innovadora y creativa, y expandiendo la literatura latinoamericana a un público más amplio y global.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# --- Angente React Basico ---\n",
    "\n",
    "# Herramienta de búsqueda en vector DB (usa tu función retrieve/rag_answer)\n",
    "def buscar_en_leyes(query: str) -> str:\n",
    "    \"\"\"Busca en la base de datos legal usando RAG.\"\"\"\n",
    "    res = rag_answer(query, k=3)\n",
    "    return f\"Respuesta (RAG): {res['answer']}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"BusquedaLegalRAG\",\n",
    "        func=buscar_en_leyes,\n",
    "        description=\"Usa esta herramienta cuando la pregunta se relacione con leyes o documentos cargados.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"LLMGeneral\",\n",
    "        func=lambda q: llm.invoke(q).content,\n",
    "        description=\"Usa esta herramienta cuando la pregunta NO sea legal o no requiera buscar documentos.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Inicializamos agente con tipo ReAct ---\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,   # muestra el razonamiento paso a paso\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "# --- Ejemplo de uso ---\n",
    "print(\"Ejemplo 1: Pregunta legal (usa RAG)\")\n",
    "respuesta1 = agent.run(\"¿Qué artículo protege la libertad de expresión en Colombia?\")\n",
    "print(respuesta1)\n",
    "\n",
    "print(\"\\nEjemplo 2: Pregunta general (usa LLM)\")\n",
    "respuesta2 = agent.run(\"¿Quién fue Gabriel García Márquez?\")\n",
    "print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba34132-bc2c-4374-9648-0c2e5cbaa9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Definido rag_answer y recreado 'agent' con la herramienta RAG. Prueba con agent.run(...)\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA: Definir retrieve + rag_answer + re-crear agente ReAct =====\n",
    "import textwrap\n",
    "import traceback\n",
    "\n",
    "# 1) Comprobaciones básicas\n",
    "if 'llm' not in globals():\n",
    "    raise NameError(\"No existe la variable 'llm'. Inicializa tu LLM (ChatGroq/ChatOpenAI) antes de ejecutar esta celda.\")\n",
    "\n",
    "has_vectordb = ('vectordb' in globals() and globals()['vectordb'] is not None)\n",
    "has_chunks = ('DOCUMENT_CHUNKS' in globals() and bool(DOCUMENT_CHUNKS))\n",
    "\n",
    "# 2) Funciones de recuperación (vectordb o fallback por keywords)\n",
    "def retrieve_by_keywords(query: str, k: int = 4):\n",
    "    \"\"\"Fallback simple: cuenta ocurrencias en DOCUMENT_CHUNKS.\"\"\"\n",
    "    if not has_chunks:\n",
    "        return []\n",
    "    q = query.lower()\n",
    "    scored = []\n",
    "    for c in DOCUMENT_CHUNKS:\n",
    "        text = c.get('text','') or ''\n",
    "        score = text.lower().count(q)  # simple heurística\n",
    "        scored.append({'text': text, 'score': score, 'metadata': {'source': c.get('source'), 'chunk_index': c.get('chunk_index')}})\n",
    "    scored = sorted(scored, key=lambda x: x['score'], reverse=True)\n",
    "    return scored[:k]\n",
    "\n",
    "def retrieve_from_vectordb(query: str, k: int = 4):\n",
    "    \"\"\"Intentar usar vectordb (Chroma/FAISS) si está disponible.\"\"\"\n",
    "    try:\n",
    "        # preferimos similarity_search_with_relevance_scores si existe\n",
    "        if hasattr(vectordb, \"similarity_search_with_relevance_scores\"):\n",
    "            results = vectordb.similarity_search_with_relevance_scores(query, k=k)\n",
    "            out = []\n",
    "            for doc, score in results:\n",
    "                text = getattr(doc, \"page_content\", None) or (doc.get('text') if isinstance(doc, dict) else str(doc))\n",
    "                meta = getattr(doc, \"metadata\", None) or (doc if isinstance(doc, dict) else {})\n",
    "                out.append({'text': text, 'score': float(score), 'metadata': meta})\n",
    "            return out\n",
    "        else:\n",
    "            results = vectordb.similarity_search(query, k=k)\n",
    "            out = []\n",
    "            for r in results:\n",
    "                text = getattr(r, \"page_content\", None) or (r.get('text') if isinstance(r, dict) else str(r))\n",
    "                meta = getattr(r, \"metadata\", None) or (r if isinstance(r, dict) else {})\n",
    "                out.append({'text': text, 'score': None, 'metadata': meta})\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Error buscando en vectordb:\", e)\n",
    "        print(\"Se usará fallback por keywords (DOCUMENT_CHUNKS).\")\n",
    "        return retrieve_by_keywords(query, k=k)\n",
    "\n",
    "def retrieve(query: str, k: int = 4):\n",
    "    return retrieve_from_vectordb(query, k) if has_vectordb else retrieve_by_keywords(query, k)\n",
    "\n",
    "# 3) Wrapper LLM compatible con .invoke() y llamada directa\n",
    "def llm_call(prompt: str, **kwargs) -> str:\n",
    "    \"\"\"Llama al LLM y devuelve texto; maneja distintos formatos de respuesta.\"\"\"\n",
    "    try:\n",
    "        resp = llm.invoke(prompt, **kwargs)\n",
    "        return getattr(resp, \"content\", None) or str(resp)\n",
    "    except Exception:\n",
    "        try:\n",
    "            resp = llm(prompt, **kwargs)\n",
    "            return getattr(resp, \"content\", None) or str(resp)\n",
    "        except Exception as e:\n",
    "            return f\"[ERROR en LLM]: {e}\"\n",
    "\n",
    "# 4) rag_answer robusto: recupera, arma contexto y llama al LLM\n",
    "def rag_answer(query: str, k: int = 4):\n",
    "    docs = retrieve(query, k)\n",
    "    if not docs:\n",
    "        return {'answer': 'No se encontraron documentos relevantes.', 'citations': []}\n",
    "\n",
    "    ctx_parts = []\n",
    "    citations = []\n",
    "    for d in docs:\n",
    "        text = d.get('text') if isinstance(d, dict) else str(d)\n",
    "        meta = d.get('metadata', {}) if isinstance(d, dict) else {}\n",
    "        src = meta.get('source', meta.get('filename', 'unknown'))\n",
    "        idx = meta.get('chunk_index')\n",
    "        score = d.get('score')\n",
    "        citations.append({'source': src, 'chunk_index': idx, 'score': score})\n",
    "        snippet = textwrap.shorten(text, width=1000, placeholder='...')\n",
    "        ctx_parts.append(f\"Source: {src} (chunk {idx}, score {score})\\n{snippet}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(ctx_parts)\n",
    "    final_prompt = (\n",
    "        \"Eres un asistente legal. Usa SOLO el contexto provisto para responder la pregunta. \"\n",
    "        \"Responde en español, de forma concisa, y al final incluye las citas en formato [source:filename chunk_index].\\n\\n\"\n",
    "        f\"Contexto:\\n{context}\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Respuesta:\"\n",
    "    )\n",
    "\n",
    "    answer_text = llm_call(final_prompt)\n",
    "    return {'answer': answer_text, 'citations': citations}\n",
    "\n",
    "# 5) Reconstruir agente ReAct usando la herramienta que llama a rag_answer\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "def buscar_en_leyes_tool(q: str) -> str:\n",
    "    try:\n",
    "        r = rag_answer(q, k=3)\n",
    "        return r.get('answer', '')\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR herramienta RAG]: {e}\"\n",
    "\n",
    "def llm_general_tool(q: str) -> str:\n",
    "    try:\n",
    "        return llm_call(q)\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR herramienta LLMGeneral]: {e}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"BusquedaLegalRAG\", func=buscar_en_leyes_tool,\n",
    "         description=\"Buscar en documentos legales indexados. Útil para consultas concretas sobre leyes y artículos.\"),\n",
    "    Tool(name=\"LLMGeneral\", func=llm_general_tool,\n",
    "         description=\"Responder con el LLM sin usar la base de datos (general knowledge).\")\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"✅ Definido rag_answer y recreado 'agent' con la herramienta RAG. Prueba con agent.run(...)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b739bab-ba98-414a-b81c-193daf06f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar un artículo legal que mencione la protección de la libertad de expresión en Colombia.\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: \"libertad de expresión Colombia\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLa Constitución Política de Colombia de 1991 establece que el Estado colombiano es democrático, participativo y pluralista, y que garantiza la dignidad humana, el trabajo y la solidaridad de las personas que lo integran (Artículo 1). Además, el artículo 20 de la Constitución establece que:\n",
      "\n",
      "\"La libertad de expresión consagrada en el artículo 19 de la Declaración Universal de los Derechos Humanos, comprende la libertad de buscar, recibir y difundir informaciones y opiniones de cualquier clase, así como la libertad de difundir ideas y opiniones sin sujeción a previa censura, sin embargo, el Estado garantiza el derecho a la información y a la protección de fuentes de información y periodistas, en el marco de sus atribuciones constitucionales y legales\".\n",
      "\n",
      "[source:Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk_4 score_1]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: ¿Qué artículo protege la libertad de expresión en Colombia?\n",
      "Thought: Pensé que la respuesta estaba en la Constitución Política de Colombia, pero ahora necesito saber cuál es el artículo específico que protege la libertad de expresión en Colombia.\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: \"artículo que protege la libertad de expresión en Colombia\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEl artículo que protege la libertad de expresión en Colombia no se menciona explícitamente en el contexto proporcionado. Sin embargo, se puede inferir que los artículos 2 y 3, que se refieren a la protección de los derechos y libertades, y a la soberanía del pueblo, pueden ser relevantes para la protección de la libertad de expresión.\n",
      "\n",
      "En particular, el artículo 2 establece que el Estado debe garantizar la efectividad de los principios, derechos y deberes consagrados en la Constitución, lo que podría incluir la libertad de expresión.\n",
      "\n",
      "[Source: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf (chunk 2, chunk_index 2)]\n",
      "[Source: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf (chunk 3, chunk_index 3)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: ¿Qué artículo protege la libertad de expresión en Colombia?\n",
      "Thought: Pensé que la respuesta estaba en la Constitución Política de Colombia, pero ahora necesito saber cuál es el artículo específico que protege la libertad de expresión en Colombia.\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: \"artículo que protege la libertad de expresión en Colombia\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo existe un artículo específico en la Constitución Política de Colombia de 1991 que proteja explícitamente la libertad de expresión. Sin embargo, el artículo 20 de la Constitución establece que \"Todas las personas tienen derecho a la libertad de expresión y de información, en forma oral o escrita, y en cualquier otro medio de comunicación\" ([source:Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf, chunk 3, score 0]).\n",
      "\n",
      "[No disponible, el archivo proporcionado no tiene chunk 3. Se debe considerar que este artículo no existe en el archivo proporcionado. Sin embargo, el artículo 20 sí existe en la Constitución Política de Colombia de 1991, pero no está disponible en el archivo proporcionado. En tal caso, se debe considerar que el artículo que protege la libertad de expresión es el artículo 20 de la Constitución Política de Colombia de 1991].\n",
      "\n",
      "[Nota: El artículo 20 es el artículo correcto, pero no está disponible en el archivo proporcionado. Se debe considerar que el artículo 20 es el artículo correcto para proteger la libertad de expresión en Colombia].\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a esta pregunta, puedo utilizar el conocimiento general o buscar en documentos legales indexados para obtener una respuesta precisa y actualizada.\n",
      "\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: habeas corpus\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSegún el artículo 30 de la Constitución Política de Colombia de 1991, el habeas corpus es un derecho que tiene quien se encuentre privado de su libertad y crea que está siendo detenido ilegalmente. Este derecho puede ser invocado ante cualquier autoridad judicial en todo tiempo, por si o por interpuesta persona, y debe resolverse en un plazo de treinta y seis horas.\n",
      "\n",
      "El Defensor del Pueblo también tiene la facultad de invocar el derecho de habeas corpus e interponer acciones de tutela, sin perjuicio del derecho que asiste a los interesados. [source:Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk_48]\n",
      "\n",
      "Citas:\n",
      "\n",
      "* [source:Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk_48]\n",
      "* [source:Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk_627]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Resume brevemente el concepto de habeas corpus.\n",
      "Thought: Para confirmar la información anterior y verificar si hay alguna otra fuente que respalde o contradiga esta información, puedo utilizar la búsqueda legal de nuevo y esta vez consultar otros documentos legales.\n",
      "\n",
      "Action: BusquedaLegalRAG\n",
      "Action Input: habeas corpus y Constitución Política de Colombia de 1991\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEl habeas corpus no está explícitamente mencionado en la Constitución Política de Colombia de 1991, sin embargo, se puede inferir su reconocimiento en el artículo 5, que establece que \"Todas las personas tienen derecho a la libertad personal, que incluye la libertad de locomoción y la inviolabilidad del hogar\".\n",
      "\n",
      "Además, el artículo 29 de la Constitución establece que \"El derecho de petición, la libertad de expresión, la libertad de reunión y la libertad de asociación son fundamentales en una sociedad democrática\".\n",
      "\n",
      "[Source: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk 4, score 0, artículo 5]\n",
      "\n",
      "[Source: Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf chunk 7, score 0, artículo 29]\n",
      "\n",
      "Es importante destacar que, a pesar de que el habeas corpus no esté explícitamente mencionado, su reconocimiento puede inferirse en la protección de la libertad personal y la inviolabilidad del hogar, que son principios fundamentales en una sociedad democrática.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Dado que la búsqueda legal no ha proporcionado una respuesta clara y concisa, puedo intentar utilizar el conocimiento general para resumir brevemente el concepto de habeas corpus.\n",
      "\n",
      "Action: LLMGeneral\n",
      "Action Input: habeas corpus\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mHabeas corpus is a fundamental legal concept that has been in place for centuries. It is a Latin phrase that translates to \"you have the body.\" In modern times, it is a writ (a formal order) issued by a court, typically a high court or a superior court, to demand that a person be brought before the court to determine whether their detention or imprisonment is lawful.\n",
      "\n",
      "History of Habeas Corpus:\n",
      "\n",
      "The concept of habeas corpus originated in ancient Rome, where it was known as \"habeas corpus ad subjiciendum.\" It was first established in English law in the 12th century, during the reign of King Henry I. The writ was formalized in the 13th century by the English Magna Carta (1215) and the Habeas Corpus Act of 1679.\n",
      "\n",
      "Purpose of Habeas Corpus:\n",
      "\n",
      "The primary purpose of habeas corpus is to prevent the arbitrary detention or imprisonment of individuals by the state or other authorities. It ensures that anyone who is detained or imprisoned is given a fair and public trial, and that the detention or imprisonment is based on a valid legal reason. The writ also provides a mechanism for challenging the legality of detention or imprisonment, and for seeking release from custody.\n",
      "\n",
      "Key Features of Habeas Corpus:\n",
      "\n",
      "1. **Right to a hearing**: Habeas corpus provides a right to a hearing before a court, where the detained person can challenge the legality of their detention or imprisonment.\n",
      "2. **Protection against arbitrary detention**: The writ protects individuals from arbitrary detention or imprisonment by the state or other authorities.\n",
      "3. **Due process**: Habeas corpus ensures that individuals are given due process of law, including a fair and public trial.\n",
      "4. **Review of detention or imprisonment**: The writ allows for the review of detention or imprisonment to determine whether it is lawful.\n",
      "\n",
      "Examples of Habeas Corpus:\n",
      "\n",
      "1. **United States vs. Nixon** (1974): The US Supreme Court issued a writ of habeas corpus to President Richard Nixon, requiring him to turn over tapes and documents related to the Watergate scandal.\n",
      "2. **Miranda vs. Arizona** (1966): The US Supreme Court held that a suspect had the right to counsel and that the police had to inform them of their rights, including the right to remain silent, in order to obtain a confession.\n",
      "3. **Bush administration's detention of terrorism suspects** (2001-2008): The US Supreme Court issued several writs of habeas corpus, challenging the detention of terrorism suspects at Guantanamo Bay and other locations.\n",
      "\n",
      "In conclusion, habeas corpus is a fundamental right that protects individuals from arbitrary detention or imprisonment by the state or other authorities. It ensures that anyone who is detained or imprisoned is given a fair and public trial, and that the detention or imprisonment is based on a valid legal reason.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "# Pruebas rápidas\n",
    "print(agent.run(\"¿Qué artículo protege la libertad de expresión en Colombia?\"))\n",
    "print(agent.run(\"Resume brevemente el concepto de habeas corpus.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
