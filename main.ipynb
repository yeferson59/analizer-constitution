{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a633aa",
   "metadata": {},
   "source": [
    "# Asistente Legal (LangChain + LLM)\n",
    "\n",
    "Este notebook implementa el proyecto: \"Asistente legal / normativo\" para consultar leyes colombianas.\n",
    "\n",
    "Objetivo de esta sección: preparar el entorno e importar las librerías necesarias. Sigue las celdas en orden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea98b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m138 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m121 packages\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add \"langchain[openai]\" dotenv ipywidgets chromadb faiss-cpu PyPDF2\n",
    "\n",
    "## installar dependencias con pip\n",
    "## %pip install -q \"langchain[openai]\" python-dotenv ipywidgets chromadb faiss-cpu PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edf3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración inicial\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt template de ejemplo\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Inicialización del LLM (no ejecutar si no tiene las variables de entorno configuradas)\n",
    "llm = ChatOpenAI(\n",
    "    api_key=getenv(\"OPENAPI_API_KEY\"),\n",
    "    base_url=getenv(\"OPENAPI_BASE_URL\"),\n",
    "    model=\"google/gemini-2.5-flash-lite-preview-09-2025\",\n",
    ")\n",
    "\n",
    "# Ejemplo de uso (descomente si tiene las claves configuradas)\n",
    "# question = \"¿Qué equipo de la NFL ganó el Super Bowl en el año en que nació Justin Bieber?\"\n",
    "# print(llm.invoke(input=question, config={\"prompt\": prompt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db183160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d2dabd3ec54b93958b29416b15cfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Sube archivos legales (.pdf, .txt, .md)'), FileUpload(value=(), accept='.txt,.md,.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6048a2352b3f493d89d741066624fe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc4578ba1584d398891d763044f071b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Previsualizar archivos', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loaders: widget FileUpload y helpers para parsear archivos subidos y leer `data/`\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# Widget para subir archivos\n",
    "uploader = widgets.FileUpload(accept='.txt,.md,.pdf', multiple=True)\n",
    "display(widgets.VBox([widgets.Label(\"Sube archivos legales (.pdf, .txt, .md)\"), uploader]))\n",
    "\n",
    "# Output para previsualización\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "\n",
    "def parse_uploaded_files(uploader_widget):\n",
    "    \"\"\"Devuelve una lista de dicts {'filename', 'text'} extraídos de los archivos subidos.\"\"\"\n",
    "    docs = []\n",
    "    # La estructura de uploader.value depende del frontend; manejar ambos casos\n",
    "    items = getattr(uploader_widget, 'value', {}) or {}\n",
    "    # En algunos entornos items es lista, en otros dict\n",
    "    try:\n",
    "        iterator = items.items()\n",
    "    except Exception:\n",
    "        # intentamos tratar como lista\n",
    "        iterator = [(f.get('name', f\"file_{i}\"), f) for i, f in enumerate(items)]\n",
    "\n",
    "    for name, fileinfo in iterator:\n",
    "        # fileinfo puede ser dict con 'content' o directamente bytes\n",
    "        content = fileinfo.get('content') if isinstance(fileinfo, dict) else fileinfo\n",
    "        text = ''\n",
    "        if name.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                from PyPDF2 import PdfReader\n",
    "                reader = PdfReader(BytesIO(content))\n",
    "                pages = [p.extract_text() or '' for p in reader.pages]\n",
    "                text = '\\n'.join(pages)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] No se pudo leer PDF ({name}): {e}\"\n",
    "        else:\n",
    "            try:\n",
    "                if isinstance(content, bytes):\n",
    "                    text = content.decode('utf-8')\n",
    "                else:\n",
    "                    text = str(content)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] decodificando {name}: {e}\"\n",
    "        docs.append({'filename': name, 'text': text})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def on_preview_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        docs = parse_uploaded_files(uploader)\n",
    "        if not docs:\n",
    "            print('No se han encontrado archivos subidos.')\n",
    "            return\n",
    "        for d in docs:\n",
    "            print(f\"--- {d['filename']} ---\")\n",
    "            print(d['text'][:1500])\n",
    "            print('\\n')\n",
    "\n",
    "preview_btn = widgets.Button(description='Previsualizar archivos')\n",
    "preview_btn.on_click(on_preview_clicked)\n",
    "display(preview_btn)\n",
    "\n",
    "\n",
    "def load_local_docs(data_dir='data'):\n",
    "    \"\"\"Lee archivos de `data/` y devuelve lista de dicts {'filename','text'}\"\"\"\n",
    "    docs = []\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"No existe la carpeta '{data_dir}'. Crea la carpeta y añade documentos de ejemplo.\")\n",
    "        return docs\n",
    "    for fn in sorted(os.listdir(data_dir)):\n",
    "        path = os.path.join(data_dir, fn)\n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "        text = ''\n",
    "        if fn.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                from PyPDF2 import PdfReader\n",
    "                reader = PdfReader(path)\n",
    "                pages = [p.extract_text() or '' for p in reader.pages]\n",
    "                text = '\\n'.join(pages)\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] leyendo PDF {fn}: {e}\"\n",
    "        else:\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                text = f\"[ERROR] leyendo {fn}: {e}\"\n",
    "        docs.append({'filename': fn, 'text': text})\n",
    "    return docs\n",
    "\n",
    "# Ejemplo de uso: docs = load_local_docs('data')\n",
    "# print(docs[0]['text'][:500]) if docs else print('No hay documentos en data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45c55f",
   "metadata": {},
   "source": [
    "## Sección: Loaders (carga de documentos)\n",
    "\n",
    "En esta sección implementaremos los loaders que permiten: (1) subir archivos locales mediante un widget, y (2) cargar archivos desde la carpeta `data/` del repositorio. Más adelante convertiremos estos textos en chunks y los indexaremos en una vector DB.\n",
    "\n",
    "Instrucciones rápidas:\n",
    "- Usa el widget para subir `.pdf`, `.txt` o `.md`.\n",
    "- Pulsa \"Previsualizar archivos\" para ver el texto extraído (o errores si falta una dependencia de PDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a8d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object ID 7606,0 ref repaired\n",
      "Object ID 1,0 ref repaired\n",
      "Object ID 2,0 ref repaired\n",
      "Object ID 7582,0 ref repaired\n",
      "Object ID 7584,0 ref repaired\n",
      "Object ID 7583,0 ref repaired\n",
      "Object ID 7589,0 ref repaired\n",
      "Object ID 7591,0 ref repaired\n",
      "Object ID 7590,0 ref repaired\n",
      "Object ID 7596,0 ref repaired\n",
      "Object ID 7598,0 ref repaired\n",
      "Object ID 7597,0 ref repaired\n",
      "Object ID 4,0 ref repaired\n",
      "Object ID 7604,0 ref repaired\n",
      "Object ID 6,0 ref repaired\n",
      "Object ID 8,0 ref repaired\n",
      "Object ID 10,0 ref repaired\n",
      "Object ID 12,0 ref repaired\n",
      "Object ID 14,0 ref repaired\n",
      "Object ID 16,0 ref repaired\n",
      "Object ID 18,0 ref repaired\n",
      "Object ID 20,0 ref repaired\n",
      "Object ID 22,0 ref repaired\n",
      "Object ID 24,0 ref repaired\n",
      "Object ID 26,0 ref repaired\n",
      "Object ID 28,0 ref repaired\n",
      "Object ID 30,0 ref repaired\n",
      "Object ID 32,0 ref repaired\n",
      "Object ID 34,0 ref repaired\n",
      "Object ID 36,0 ref repaired\n",
      "Object ID 38,0 ref repaired\n",
      "Object ID 40,0 ref repaired\n",
      "Object ID 42,0 ref repaired\n",
      "Object ID 44,0 ref repaired\n",
      "Object ID 46,0 ref repaired\n",
      "Object ID 48,0 ref repaired\n",
      "Object ID 50,0 ref repaired\n",
      "Object ID 52,0 ref repaired\n",
      "Object ID 54,0 ref repaired\n",
      "Object ID 56,0 ref repaired\n",
      "Object ID 58,0 ref repaired\n",
      "Object ID 60,0 ref repaired\n",
      "Object ID 62,0 ref repaired\n",
      "Object ID 64,0 ref repaired\n",
      "Object ID 66,0 ref repaired\n",
      "Object ID 68,0 ref repaired\n",
      "Object ID 70,0 ref repaired\n",
      "Object ID 72,0 ref repaired\n",
      "Object ID 74,0 ref repaired\n",
      "Object ID 76,0 ref repaired\n",
      "Object ID 78,0 ref repaired\n",
      "Object ID 80,0 ref repaired\n",
      "Object ID 82,0 ref repaired\n",
      "Object ID 84,0 ref repaired\n",
      "Object ID 86,0 ref repaired\n",
      "Object ID 88,0 ref repaired\n",
      "Object ID 90,0 ref repaired\n",
      "Object ID 92,0 ref repaired\n",
      "Object ID 94,0 ref repaired\n",
      "Object ID 96,0 ref repaired\n",
      "Object ID 98,0 ref repaired\n",
      "Object ID 100,0 ref repaired\n",
      "Object ID 102,0 ref repaired\n",
      "Object ID 104,0 ref repaired\n",
      "Object ID 106,0 ref repaired\n",
      "Object ID 108,0 ref repaired\n",
      "Object ID 110,0 ref repaired\n",
      "Object ID 112,0 ref repaired\n",
      "Object ID 114,0 ref repaired\n",
      "Object ID 116,0 ref repaired\n",
      "Object ID 118,0 ref repaired\n",
      "Object ID 120,0 ref repaired\n",
      "Object ID 122,0 ref repaired\n",
      "Object ID 124,0 ref repaired\n",
      "Object ID 126,0 ref repaired\n",
      "Object ID 128,0 ref repaired\n",
      "Object ID 130,0 ref repaired\n",
      "Object ID 132,0 ref repaired\n",
      "Object ID 134,0 ref repaired\n",
      "Object ID 136,0 ref repaired\n",
      "Object ID 138,0 ref repaired\n",
      "Object ID 140,0 ref repaired\n",
      "Object ID 142,0 ref repaired\n",
      "Object ID 144,0 ref repaired\n",
      "Object ID 146,0 ref repaired\n",
      "Object ID 148,0 ref repaired\n",
      "Object ID 150,0 ref repaired\n",
      "Object ID 152,0 ref repaired\n",
      "Object ID 154,0 ref repaired\n",
      "Object ID 156,0 ref repaired\n",
      "Object ID 158,0 ref repaired\n",
      "Object ID 160,0 ref repaired\n",
      "Object ID 162,0 ref repaired\n",
      "Object ID 164,0 ref repaired\n",
      "Object ID 166,0 ref repaired\n",
      "Object ID 168,0 ref repaired\n",
      "Object ID 170,0 ref repaired\n",
      "Object ID 172,0 ref repaired\n",
      "Object ID 174,0 ref repaired\n",
      "Object ID 176,0 ref repaired\n",
      "Object ID 178,0 ref repaired\n",
      "Object ID 180,0 ref repaired\n",
      "Object ID 182,0 ref repaired\n",
      "Object ID 184,0 ref repaired\n",
      "Object ID 186,0 ref repaired\n",
      "Object ID 188,0 ref repaired\n",
      "Object ID 190,0 ref repaired\n",
      "Object ID 192,0 ref repaired\n",
      "Object ID 194,0 ref repaired\n",
      "Object ID 196,0 ref repaired\n",
      "Object ID 198,0 ref repaired\n",
      "Object ID 200,0 ref repaired\n",
      "Object ID 202,0 ref repaired\n",
      "Object ID 204,0 ref repaired\n",
      "Object ID 206,0 ref repaired\n",
      "Object ID 208,0 ref repaired\n",
      "Object ID 210,0 ref repaired\n",
      "Object ID 212,0 ref repaired\n",
      "Object ID 214,0 ref repaired\n",
      "Object ID 216,0 ref repaired\n",
      "Object ID 218,0 ref repaired\n",
      "Object ID 220,0 ref repaired\n",
      "Object ID 222,0 ref repaired\n",
      "Object ID 224,0 ref repaired\n",
      "Object ID 226,0 ref repaired\n",
      "Object ID 228,0 ref repaired\n",
      "Object ID 230,0 ref repaired\n",
      "Object ID 232,0 ref repaired\n",
      "Object ID 234,0 ref repaired\n",
      "Object ID 236,0 ref repaired\n",
      "Object ID 238,0 ref repaired\n",
      "Object ID 240,0 ref repaired\n",
      "Object ID 242,0 ref repaired\n",
      "Object ID 244,0 ref repaired\n",
      "Object ID 246,0 ref repaired\n",
      "Object ID 248,0 ref repaired\n",
      "Object ID 250,0 ref repaired\n",
      "Object ID 252,0 ref repaired\n",
      "Object ID 254,0 ref repaired\n",
      "Object ID 256,0 ref repaired\n",
      "Object ID 258,0 ref repaired\n",
      "Object ID 260,0 ref repaired\n",
      "Object ID 262,0 ref repaired\n",
      "Object ID 264,0 ref repaired\n",
      "Object ID 266,0 ref repaired\n",
      "Object ID 268,0 ref repaired\n",
      "Object ID 270,0 ref repaired\n",
      "Object ID 272,0 ref repaired\n",
      "Object ID 274,0 ref repaired\n",
      "Object ID 276,0 ref repaired\n",
      "Object ID 278,0 ref repaired\n",
      "Object ID 280,0 ref repaired\n",
      "Object ID 282,0 ref repaired\n",
      "Object ID 284,0 ref repaired\n",
      "Object ID 286,0 ref repaired\n",
      "Object ID 288,0 ref repaired\n",
      "Object ID 290,0 ref repaired\n",
      "Object ID 292,0 ref repaired\n",
      "Object ID 294,0 ref repaired\n",
      "Object ID 296,0 ref repaired\n",
      "Object ID 298,0 ref repaired\n",
      "Object ID 300,0 ref repaired\n",
      "Object ID 302,0 ref repaired\n",
      "Object ID 304,0 ref repaired\n",
      "Object ID 306,0 ref repaired\n",
      "Object ID 308,0 ref repaired\n",
      "Object ID 310,0 ref repaired\n",
      "Object ID 312,0 ref repaired\n",
      "Object ID 314,0 ref repaired\n",
      "Object ID 316,0 ref repaired\n",
      "Object ID 318,0 ref repaired\n",
      "Object ID 320,0 ref repaired\n",
      "Object ID 322,0 ref repaired\n",
      "Object ID 324,0 ref repaired\n",
      "Object ID 326,0 ref repaired\n",
      "Object ID 328,0 ref repaired\n",
      "Object ID 330,0 ref repaired\n",
      "Object ID 332,0 ref repaired\n",
      "Object ID 334,0 ref repaired\n",
      "Object ID 336,0 ref repaired\n",
      "Object ID 338,0 ref repaired\n",
      "Object ID 340,0 ref repaired\n",
      "Object ID 342,0 ref repaired\n",
      "Object ID 344,0 ref repaired\n",
      "Object ID 346,0 ref repaired\n",
      "Object ID 348,0 ref repaired\n",
      "Object ID 350,0 ref repaired\n",
      "Object ID 352,0 ref repaired\n",
      "Object ID 354,0 ref repaired\n",
      "Object ID 356,0 ref repaired\n",
      "Object ID 358,0 ref repaired\n",
      "Object ID 360,0 ref repaired\n",
      "Object ID 362,0 ref repaired\n",
      "Object ID 364,0 ref repaired\n",
      "Object ID 366,0 ref repaired\n",
      "Object ID 368,0 ref repaired\n",
      "Object ID 370,0 ref repaired\n",
      "Object ID 372,0 ref repaired\n",
      "Object ID 374,0 ref repaired\n",
      "Object ID 376,0 ref repaired\n",
      "Object ID 378,0 ref repaired\n",
      "Object ID 380,0 ref repaired\n",
      "Object ID 382,0 ref repaired\n",
      "Object ID 384,0 ref repaired\n",
      "Object ID 386,0 ref repaired\n",
      "Object ID 388,0 ref repaired\n",
      "Object ID 390,0 ref repaired\n",
      "Object ID 392,0 ref repaired\n",
      "Object ID 394,0 ref repaired\n",
      "Object ID 396,0 ref repaired\n",
      "Object ID 398,0 ref repaired\n",
      "Object ID 400,0 ref repaired\n",
      "Object ID 402,0 ref repaired\n",
      "Object ID 404,0 ref repaired\n",
      "Object ID 406,0 ref repaired\n",
      "Object ID 408,0 ref repaired\n",
      "Object ID 410,0 ref repaired\n",
      "Object ID 412,0 ref repaired\n",
      "Object ID 414,0 ref repaired\n",
      "Object ID 416,0 ref repaired\n",
      "Object ID 418,0 ref repaired\n",
      "Object ID 420,0 ref repaired\n",
      "Object ID 422,0 ref repaired\n",
      "Object ID 424,0 ref repaired\n",
      "Object ID 426,0 ref repaired\n",
      "Object ID 428,0 ref repaired\n",
      "Object ID 430,0 ref repaired\n",
      "Object ID 432,0 ref repaired\n",
      "Object ID 434,0 ref repaired\n",
      "Object ID 436,0 ref repaired\n",
      "Object ID 438,0 ref repaired\n",
      "Object ID 440,0 ref repaired\n",
      "Object ID 442,0 ref repaired\n",
      "Object ID 444,0 ref repaired\n",
      "Object ID 446,0 ref repaired\n",
      "Object ID 448,0 ref repaired\n",
      "Object ID 450,0 ref repaired\n",
      "Object ID 452,0 ref repaired\n",
      "Object ID 454,0 ref repaired\n",
      "Object ID 456,0 ref repaired\n",
      "Object ID 458,0 ref repaired\n",
      "Object ID 460,0 ref repaired\n",
      "Object ID 462,0 ref repaired\n",
      "Object ID 464,0 ref repaired\n",
      "Object ID 466,0 ref repaired\n",
      "Object ID 468,0 ref repaired\n",
      "Object ID 470,0 ref repaired\n",
      "Object ID 472,0 ref repaired\n",
      "Object ID 474,0 ref repaired\n",
      "Object ID 476,0 ref repaired\n",
      "Object ID 478,0 ref repaired\n",
      "Object ID 480,0 ref repaired\n",
      "Object ID 482,0 ref repaired\n",
      "Object ID 484,0 ref repaired\n",
      "Object ID 486,0 ref repaired\n",
      "Object ID 488,0 ref repaired\n",
      "Object ID 490,0 ref repaired\n",
      "Object ID 492,0 ref repaired\n",
      "Object ID 494,0 ref repaired\n",
      "Object ID 496,0 ref repaired\n",
      "Object ID 498,0 ref repaired\n",
      "Object ID 500,0 ref repaired\n",
      "Object ID 502,0 ref repaired\n",
      "Object ID 504,0 ref repaired\n",
      "Object ID 506,0 ref repaired\n",
      "Object ID 508,0 ref repaired\n",
      "Object ID 510,0 ref repaired\n",
      "Object ID 512,0 ref repaired\n",
      "Object ID 514,0 ref repaired\n",
      "Object ID 516,0 ref repaired\n",
      "Object ID 518,0 ref repaired\n",
      "Object ID 520,0 ref repaired\n",
      "Object ID 522,0 ref repaired\n",
      "Object ID 524,0 ref repaired\n",
      "Object ID 526,0 ref repaired\n",
      "Object ID 528,0 ref repaired\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos cargados: 2\n",
      "Total chunks generados: 1215\n",
      " - Constitución_Política_1_de_1991_Asamblea_Nacional_Constituyente.pdf: 1050 chunks\n",
      " - reglamento_academi_pregrado.pdf: 165 chunks\n",
      "\n",
      "Ejemplo (primer chunk, 500 chars):\n",
      "Departamento Administrativo de la Función Pública\n",
      "Constitución Política 1 de 1991 Asamblea\n",
      "Nacional Constituyente1 EVA - Gestor Normativo\n",
      "Constitución Política 1 de 1991 Asamblea Nacional\n",
      "Constituyente\n",
      "Los datos publicados tienen propósitos exclusivamente informativos. El Departamento Administrativo de la Función Pública no se hace\n",
      "responsable de la vigencia de la presente norma. Nos encontramos en un proceso permanente de actualización de los contenidos.\n",
      "CONSTITUCIÓN POLITICA DE LA REPUBLICA DE\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento: splitters y creación de chunks\n",
    "# Carga documentos desde `data/`, aplica un splitter y guarda los chunks en `DOCUMENT_CHUNKS`.\n",
    "from collections import Counter\n",
    "\n",
    "# Intentar importar el splitter de LangChain; si la API cambia, manejar el fallback\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "except Exception:\n",
    "    try:\n",
    "        # versión alternativa\n",
    "        from langchain.text_splitter import CharacterTextSplitter as RecursiveCharacterTextSplitter\n",
    "    except Exception:\n",
    "        RecursiveCharacterTextSplitter = None\n",
    "\n",
    "\n",
    "def create_chunks(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Devuelve lista de dicts {'source','chunk_index','text'}\"\"\"\n",
    "    if RecursiveCharacterTextSplitter is None:\n",
    "        raise ImportError(\"No se encontró RecursiveCharacterTextSplitter. Instala langchain actualizado.\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "    for d in docs:\n",
    "        text = d.get('text') or ''\n",
    "        if not text.strip():\n",
    "            # texto vacío o error al leer\n",
    "            continue\n",
    "        texts = splitter.split_text(text)\n",
    "        for i, t in enumerate(texts):\n",
    "            all_chunks.append({'source': d.get('filename', 'unknown'), 'chunk_index': i, 'text': t})\n",
    "    return all_chunks\n",
    "\n",
    "# Cargar documentos locales desde data/\n",
    "DOCUMENTS = load_local_docs('data')\n",
    "print(f\"Documentos cargados: {len(DOCUMENTS)}\")\n",
    "\n",
    "# Crear chunks\n",
    "DOCUMENT_CHUNKS = create_chunks(DOCUMENTS, chunk_size=1000, chunk_overlap=200) if DOCUMENTS else []\n",
    "print(f\"Total chunks generados: {len(DOCUMENT_CHUNKS)}\")\n",
    "\n",
    "# Mostrar conteo por documento y un ejemplo\n",
    "if DOCUMENT_CHUNKS:\n",
    "    per_doc = Counter([c['source'] for c in DOCUMENT_CHUNKS])\n",
    "    for doc, cnt in per_doc.items():\n",
    "        print(f\" - {doc}: {cnt} chunks\")\n",
    "    print('\\nEjemplo (primer chunk, 500 chars):')\n",
    "    print(DOCUMENT_CHUNKS[0]['text'][:500])\n",
    "else:\n",
    "    print('No hay chunks (documentos vacíos o problema al leer los archivos).')\n",
    "\n",
    "# Ahora `DOCUMENTS` y `DOCUMENT_CHUNKS` están listos para embeddings / indexado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb7acfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m138 packages\u001b[0m \u001b[2min 348ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m121 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add langchain-openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e4b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No se encontró OpenAIEmbeddings. Instala/actualiza langchain o ajusta el código según la versión.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m         FAISS = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m openAIEmbeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo se encontró OpenAIEmbeddings. Instala/actualiza langchain o ajusta el código según la versión.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Preparar textos y metadatos\u001b[39;00m\n\u001b[32m     39\u001b[39m texts = [c[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m DOCUMENT_CHUNKS]\n",
      "\u001b[31mImportError\u001b[39m: No se encontró OpenAIEmbeddings. Instala/actualiza langchain o ajusta el código según la versión."
     ]
    }
   ],
   "source": [
    "# Embeddings + VectorStore (Chroma preferido, FAISS fallback) y pipeline RAG\n",
    "import textwrap\n",
    "# Requerir DOCUMENT_CHUNKS\n",
    "try:\n",
    "    len(DOCUMENT_CHUNKS)\n",
    "except NameError:\n",
    "    raise NameError('DOCUMENT_CHUNKS no está definido. Ejecuta la celda de splitters antes de esta.')\n",
    "\n",
    "# Import embeddings\n",
    "openAIEmbeddings = None\n",
    "try:\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    openAIEmbeddings = OpenAIEmbeddings()\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "        openAIEmbeddings = OpenAIEmbeddings()\n",
    "    except Exception:\n",
    "        openAIEmbeddings = None\n",
    "\n",
    "# Import vectorstores\n",
    "Chroma = None\n",
    "FAISS = None\n",
    "try:\n",
    "    from langchain.vectorstores import Chroma\n",
    "    Chroma = Chroma\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.vectorstores import FAISS\n",
    "        FAISS = FAISS\n",
    "    except Exception:\n",
    "        Chroma = None\n",
    "        FAISS = None\n",
    "\n",
    "if openAIEmbeddings is None:\n",
    "    raise ImportError('No se encontró OpenAIEmbeddings. Instala/actualiza langchain o ajusta el código según la versión.')\n",
    "\n",
    "# Preparar textos y metadatos\n",
    "texts = [c['text'] for c in DOCUMENT_CHUNKS]\n",
    "metadatas = [{'source': c['source'], 'chunk_index': c['chunk_index']} for c in DOCUMENT_CHUNKS]\n",
    "\n",
    "# Inicializar embeddings\n",
    "emb = openAIEmbeddings()\n",
    "\n",
    "# Construir o cargar vectorstore\n",
    "vectordb = None\n",
    "try:\n",
    "    if Chroma is not None:\n",
    "        # persist_directory crea/usa la carpeta chroma_db\n",
    "        vectordb = Chroma.from_texts(texts, embedding=emb, metadatas=metadatas, collection_name='laws', persist_directory='chroma_db')\n",
    "        try:\n",
    "            vectordb.persist()\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        # FAISS fallback (in-memory)\n",
    "        from langchain.vectorstores import FAISS\n",
    "        vectordb = FAISS.from_texts(texts, embedding=emb, metadatas=metadatas)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Error creando vectorstore: {e}')\n",
    "\n",
    "print('Vectorstore preparado.')\n",
    "\n",
    "# Helper: recuperar documentos (devuelve lista de objetos con page_content y metadata)\n",
    "def retrieve(query, k=4):\n",
    "    # similarity_search devuelve objetos Document o similares\n",
    "    try:\n",
    "        results = vectordb.similarity_search(query, k=k)\n",
    "    except Exception:\n",
    "        # Algunas versiones usan similarity_search_with_relevance_scores\n",
    "        try:\n",
    "            results = [r[0] for r in vectordb.similarity_search_with_relevance_scores(query, k=k)]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'No se pudo ejecutar la búsqueda: {e}')\n",
    "    return results\n",
    "\n",
    "# Pipeline RAG: sintetizar respuesta con LLM usando contexto recuperado\n",
    "\n",
    "def rag_answer(query, k=4):\n",
    "    docs = retrieve(query, k=k)\n",
    "    if not docs:\n",
    "        return {'answer': 'No se encontraron documentos relevantes.', 'citations': []}\n",
    "\n",
    "    # Construir contexto concatenando fragmentos recuperados con metadata\n",
    "    ctx_parts = []\n",
    "    citations = []\n",
    "    for d in docs:\n",
    "        # compatibilidad: Document tiene page_content y metadata\n",
    "        text = getattr(d, 'page_content', None) or d.get('text') if isinstance(d, dict) else str(d)\n",
    "        meta = getattr(d, 'metadata', None) or (d if isinstance(d, dict) else {})\n",
    "        src = meta.get('source', meta.get('file_name', 'unknown'))\n",
    "        idx = meta.get('chunk_index')\n",
    "        citations.append({'source': src, 'chunk_index': idx})\n",
    "        snippet = textwrap.shorten(text, width=1000, placeholder='...')\n",
    "        ctx_parts.append(f\"Source: {src} (chunk {idx})\\n{snippet}\")\n",
    "\n",
    "    context = '\\n\\n'.join(ctx_parts)\n",
    "\n",
    "    # Prompt final\n",
    "    final_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer conciso en español. Indica al final las citas como [source:filename chunk_index].\"\n",
    "\n",
    "    # Llamada al LLM (usar llm.invoke si está disponible)\n",
    "    try:\n",
    "        resp = llm.invoke(input=final_prompt)\n",
    "        # si resp es un objeto complejo, intentar extraer texto\n",
    "        answer_text = str(resp)\n",
    "    except Exception:\n",
    "        try:\n",
    "            # fallback: si llm tiene método __call__\n",
    "            answer_text = llm(final_prompt)\n",
    "        except Exception as e:\n",
    "            answer_text = f\"[ERROR llamando al LLM: {e}]\"\n",
    "\n",
    "    return {'answer': answer_text, 'citations': citations}\n",
    "\n",
    "# Widget simple para consultas RAG\n",
    "query_box = widgets.Text(placeholder='Escribe tu pregunta sobre la ley (ej: ¿Qué artículo protege la libertad de expresión?)', description='Pregunta:', layout=widgets.Layout(width='80%'))\n",
    "ask_btn = widgets.Button(description='Consultar')\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_ask(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        q = query_box.value.strip()\n",
    "        if not q:\n",
    "            print('Escribe una pregunta primero.')\n",
    "            return\n",
    "        print('Buscando documentos relevantes...')\n",
    "        res = rag_answer(q, k=4)\n",
    "        print('\\n--- RESPUESTA ---\\n')\n",
    "        print(res['answer'])\n",
    "        print('\\n--- CITAS ---')\n",
    "        for c in res['citations']:\n",
    "            print(f\"- {c['source']} (chunk {c['chunk_index']})\")\n",
    "\n",
    "ask_btn.on_click(on_ask)\n",
    "\n",
    "display(widgets.VBox([query_box, ask_btn, out]))\n",
    "\n",
    "print('Interfaz RAG lista. Escribe una pregunta y pulsa \"Consultar\".')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analizer-constitution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
